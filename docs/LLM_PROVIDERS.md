# üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

## üìã –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã

### 1. **Gemini (Google)** - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–∏–∑–∞
- ‚úÖ –ë–æ–ª—å—à–æ–π context window (2M —Ç–æ–∫–µ–Ω–æ–≤)
- ‚úÖ Structured output –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞—è —Ü–µ–Ω–∞

### 2. **Generic HTTP Provider**
–†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º HTTP API:
- Ollama (–ª–æ–∫–∞–ª—å–Ω–æ)
- LM Studio (–ª–æ–∫–∞–ª—å–Ω–æ)
- LocalAI (–ª–æ–∫–∞–ª—å–Ω–æ)
- vLLM (–æ–±–ª–∞–∫–æ/–ª–æ–∫–∞–ª—å–Ω–æ)
- OpenAI-compatible API

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í–∞—Ä–∏–∞–Ω—Ç 1: Gemini (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

```bash
# .env
LLM_PROVIDER=gemini
LLM_MODEL=gemini-1.5-pro
API_KEY=your-google-api-key
```

**–ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á:** https://makersuite.google.com/app/apikey

### –í–∞—Ä–∏–∞–Ω—Ç 2: Ollama (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –ª–æ–∫–∞–ª—å–Ω–æ)

1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: https://ollama.com/download
2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å:
```bash
ollama pull llama3.1:8b
ollama serve
```

3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ .env:
```bash
LLM_PROVIDER=generic
LLM_FORMAT=ollama
LLM_BASE_URL=http://localhost:11434
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: LM Studio (GUI –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)

1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ LM Studio: https://lmstudio.ai/
2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ GUI
3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä (Server tab ‚Üí Start Server)

4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ .env:
```bash
LLM_PROVIDER=generic
LLM_FORMAT=openai
LLM_BASE_URL=http://localhost:1234
```

---

## ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|----------|----------|--------|
| `LLM_PROVIDER` | –¢–∏–ø –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ | `gemini` –∏–ª–∏ `generic` |

### –î–ª—è Gemini

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|----------|----------|--------|
| `LLM_MODEL` | –ú–æ–¥–µ–ª—å | `gemini-1.5-pro`, `gemini-1.5-flash` |
| `API_KEY` | API –∫–ª—é—á Google | `AIza...` |

### –î–ª—è Generic –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|----------|----------|--------|
| `LLM_BASE_URL` | –ë–∞–∑–æ–≤—ã–π URL API | `http://localhost:11434` |
| `LLM_FORMAT` | –§–æ—Ä–º–∞—Ç API | `openai`, `ollama`, `raw` |
| `API_KEY` | API –∫–ª—é—á (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω) | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ |

---

## üîß –§–æ—Ä–º–∞—Ç—ã API

### `openai` - OpenAI-compatible
–†–∞–±–æ—Ç–∞–µ—Ç —Å:
- LM Studio
- LocalAI
- vLLM (—Å OpenAI endpoint)
- Text Generation Inference

**–§–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "messages": [{"role": "user", "content": "..."}],
  "temperature": 0.2,
  "max_tokens": 2000
}
```

### `ollama` - Ollama API
–†–∞–±–æ—Ç–∞–µ—Ç —Å Ollama –ª–æ–∫–∞–ª—å–Ω–æ

**–§–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "model": "llama3.1:8b",
  "prompt": "...",
  "format": "json",
  "stream": false
}
```

### `raw` - –ü—Ä–æ—Å—Ç–æ–π JSON
–î–ª—è custom API

**–§–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "prompt": "...",
  "temperature": 0.2,
  "max_tokens": 2000
}
```

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏

### –î–ª—è production
- **Gemini 1.5 Pro** - –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Ü–µ–Ω–∞
- **Claude 3.5 Sonnet** - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ reasoning (–Ω—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä)

### –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- **Gemini 1.5 Flash** - –±—ã—Å—Ç—Ä–æ –∏ –¥—ë—à–µ–≤–æ
- **Ollama llama3.1:8b** - –±–µ—Å–ø–ª–∞—Ç–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ

### –î–ª—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏
- **Ollama** (–ª—é–±–∞—è –º–æ–¥–µ–ª—å) - –≤—Å—ë –ª–æ–∫–∞–ª—å–Ω–æ
- **LM Studio** - GUI + –ª–æ–∫–∞–ª—å–Ω–æ

---

## üìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ

### –°–æ–∑–¥–∞–Ω–∏–µ Gemini –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

```go
provider := llm.NewGeminiProvider(genkitApp, "gemini-1.5-pro")
```

### –°–æ–∑–¥–∞–Ω–∏–µ Ollama –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

```go
provider := llm.NewOllamaProvider("http://localhost:11434", "llama3.1:8b")
```

### –°–æ–∑–¥–∞–Ω–∏–µ LM Studio –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

```go
provider := llm.NewLMStudioProvider("http://localhost:1234")
```

### –°–æ–∑–¥–∞–Ω–∏–µ Generic –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

```go
provider := llm.NewGenericProvider(llm.GenericConfig{
    Name:    "my-custom-llm",
    BaseURL: "https://api.example.com",
    APIKey:  "your-key",
    Format:  llm.FormatOpenAI,
})
```

---

## üêõ Troubleshooting

### Ollama: "connection refused"
```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω
ollama serve

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
curl http://localhost:11434/api/tags
```

### LM Studio: "404 Not Found"
- –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω (Server tab ‚Üí Start Server)
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä—Ç –≤ UI (–æ–±—ã—á–Ω–æ 1234)

### Generic: "invalid JSON response"
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å markdown –≤–º–µ—Å—Ç–æ —á–∏—Å—Ç–æ–≥–æ JSON
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ —É–ª—É—á—à–∏—Ç–µ –ø—Ä–æ–º–ø—Ç
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–¥–µ—Ä–∂–∫—É JSON mode –≤ –≤–∞—à–µ–º API

---

## üí° –°–æ–≤–µ—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏** - –±—ã—Å—Ç—Ä–µ–µ –∏ –±–µ—Å–ø–ª–∞—Ç–Ω–æ
2. **Gemini –¥–ª—è production** - –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å
3. **Temperature = 0.2** - –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
4. **Max tokens = 2000** - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è structured output

---

## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Ollama](https://ollama.com/)
- [LM Studio](https://lmstudio.ai/)
- [Gemini API](https://ai.google.dev/)
- [LocalAI](https://localai.io/)
- [vLLM](https://docs.vllm.ai/)
